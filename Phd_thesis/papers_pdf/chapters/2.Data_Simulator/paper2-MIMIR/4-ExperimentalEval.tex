\section{Experimental evaluation}
\label{mimir:section:evaluationAnalysis}
This section performs an experimental evaluation of the challenges and opportunities that MIMIR-UW presents for \ac{SLAM}, depth estimation, and object segmentation algorithms.
% MIMIR-UW's challenges and opportunities under open-source algorithms for \ac{SLAM}, depth estimation, and object segmentation are experimentally evaluated.
% % Various open-source algorithms are set up to demonstrate the challenges and opportunities of MIMIR-UW under \ac{SLAM}, depth estimation, and object segmentation. 
Moreover, a sample dataset of a real-world pipeline inspection scenario (shown in Fig. \ref{mimir:fig:S2R}) is used to demonstrate MIMIR-UW's capabilities for sim-to-real transfer. This sample dataset contains manually-generated labels for segmentation, and a sparse pseudo ground truth for depth generated by the photogrammetry framework AliceVision \cite{alicevision2021}. The real pipeline scenario is not publicly available, but sample images of ground truth and results are depicted in Figs. \ref{mimir:fig:segmentation} and  \ref{mimir:fig:depth}.
%Additionally, the pseudo ground truth depth for 100 images in the real pipeline dataset is used for evaluation and are produced using the photogrammetry framework AliceVision\cite{alicevision2021}.
%Here, we describe the selected state-of-the-art algorithms and metrics and showcase the obtained results.

\begin{table}[b!]
\centering
\footnotesize
\caption{Motion diversity metric.}
\label{mimirtable:comparisonmotiondiv}
\begin{tabular}{ c  c c c  c c c  c c c}
\toprule
\multirow{2}{*}{} & \multicolumn{3}{c}{Euroc}     & \multicolumn{3}{c}{Aqualoc}     & \multicolumn{3}{c}{MIMIR} \\ %
\cmidrule(l{1.5em}r{1.5em}){2-4}
\cmidrule(l{1.5em}r{1.5em}){5-7}
\cmidrule(l{1.5em}r{1.5em}){8-10}

           & translation & rotation & total &   translation & rotation & total &   translation & rotation & total\\
\midrule
$\sigma$   & 0.334       & \textbf{0.359 }   &\textbf{ 0.347} & 0.226           & 0.067      & 0.147   & \textbf{0.684}          & 0.008     & 0.345 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[b]
    \centering
    \begin{tabular}{c@{\hspace{1.mm}} c@{\hspace{1.mm}} c@{\hspace{.5mm}} } 
         \includegraphics[width=.31\columnwidth]{figs/paper2-mimir/results/simimir/sandple3.jpg} &  \includegraphics[width=.31\columnwidth]{figs/paper2-mimir/results/simimir/seasample6.jpg}  &
         % \includegraphics[width=.45\columnwidth]{figures/results/simimir/real1.png} &  
         \includegraphics[width=.31\columnwidth]{figs/paper2-mimir/results/simimir/real2.jpg}  
    \end{tabular}

    \caption[Pipe images from SandPipe, SeaFloor, and the real scenario used for sim-to-real transfer.]{From left to right, pipe images from SandPipe, SeaFloor, and the real scenario used for sim-to-real transfer. The shape, colour, and environmental conditions for SandPipe are more similar to this sample of real-world data.}
    \label{mimir:fig:S2R}
\end{figure}
% The challenging conditions presented cause ORB-SLAM3 to fail, thus the mean duration of the estimated tracks is indicated. DSO is able to complete some of the tracks, but fails in the darkest environments.

\begin{table}[ht!]
\centering
\footnotesize
\caption[Results of deploying the proposed SLAM algorithms in MIMIR-UW]{Results of deploying the proposed SLAM algorithms in MIMIR-UW. The best results for relative errors are marked in bold.
}
\label{mimir:table:comparisonSLAM}
\begin{tabular}{ r@{\hspace{1mm}} c  c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{2mm}} c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{2mm}} c@{\hspace{1mm}}c@{\hspace{1mm}}c }
\toprule
\multirow{2}{*}[-1.2em]{Environment}  & \multirow{2}{*}[-1.2em]{Track}
& \multicolumn{3}{c}{ORB-SLAM3 (monocular)} & \multicolumn{3}{c}{ORB-SLAM3 (stereo)} & \multicolumn{3}{c}{DSO}\\ 
\cmidrule(l{1.5em}r{1.5em}){6-8}\cmidrule(l{1.5em}r{1.5em}){9-11}\cmidrule(l{1.5em}r{1.5em}){3-5}
                                &            & ATE[m] & RPE[m]  & duration[s] &  ATE[m] & RPE[m] & duration[s] &  ATE[m] & RPE[m] & SR \\
\toprule
\multirow{3}{4em}{SeaFloor} 
                                & 0       & 3.67 & \textbf{0.13}  & 34.8 $\pm$ 3.17 &  19.25  & 0.612 & 79.38 $\pm$ 15.04 & - & - & 0  \\
                                & 1       & 8.78 & 0.19  & 64.5 $\pm$ 14.9 &  9.61 & 0.497 & 86.52 $\pm$ 0.00& 2.73 & \textbf{0.0053} & 0.4 \\
                                & 2       & 5.18 & 0.142  & 40.2 $\pm$ 8.9 & 14.31 & 0.826 & 85.46 $\pm$ 22.92 & 17.06 & \textbf{0.109} & 1  \\
\midrule
\multirow{3}{4em}{SeaFloor Algae}
                                & 0     & 2.99 & 0.122   & 31.27 $\pm$ 10.4 & 14.50 & 0.687 & 75.26 $\pm$ 0.19 & 3.71 & \textbf{0.019} & 0.4 \\
                                & 1     & 1.15 & 0.134   & 53.8 $\pm$ 17.9 & 7.02 & 0.287 & 83.98 $\pm$ 4.08 & 7.00 & \textbf{0.048} & 1  \\
                                & 2     & 9.16 & 0.144   & 53.11 $\pm$ 9.7 & 2.99 & 0.47 & 64.35 $\pm$ 37.47 & 4.51 & \textbf{0.044} & 0.9 \\
\midrule
\multirow{3}{4em}{Ocean Floor}
                                & 0 dark & 5.78 & \textbf{0.523}   & 11.3 $\pm$ 3.56 & 3.89 &  0.880 & 18.47 $\pm$ 0.08 & - & - & 0 \\
                                & 0 light & 8.37 & \textbf{0.214}   & 26.7 $\pm$ 5.5 & 12.42 & 1.181 & 91.13 $\pm$ 19.19 & - & - & 0  \\
                                & 1 light & 23.66 & \textbf{0.286}   & 64.4 $\pm$ 12.8 & 57.10 & 1.575 & 172.8 $\pm$ 16.65 & - & - & 0  \\
\midrule
\multirow{2}{4em}{Sand Pipe}
                                & 0 dark  & 20.084 & \textbf{0.184}  & 83.7 $\pm$ 13.7 & 5.48 & 1.831 & 25.72 $\pm$ 8.29 & - & - & 0  \\
                                & 0 light & 6.85 & 0.0764   & 76.7 $\pm$ 14.5 & 17.72 & 5.13 & 114.64 $\pm$ 0.00& 10.64 & \textbf{0.027} & 0.7  \\
\bottomrule
\end{tabular}
\end{table}



\subsection{SLAM}
To demonstrate the provided dataset's challenges for SLAM, we run ORB-SLAM3 \cite{campos2021orb} in both a monocular and stereo fashion.
ORB-SLAM is one of the primary state-of-the-art indirect visual \ac{SLAM} algorithms. %It integrates an indirect-based approach that relies on ORB descriptors. %The tracking implements a constant velocity model for a faster search of descriptors between consecutive frames. This, altogether with the use of these binary descriptors in both the front-end and the back-end of the algorithm, allows its real-time performance. 
One of the main causes of failure for previous versions of ORB-SLAM was track loss and subsequent relocalization failure. However, since ORB-SLAM3, the back-end integrates the so-called Atlas map: every time the tracking gets lost, a new "active" map is initialized, and the previous map is stored in memory, turning into a "passive" map. During runtime, the back-end looks for loop closures on both the active and the set of passive maps available. %If a loop closure is detected in the active map, the drift error is propagated throughout the trajectory. If detected in the passive map, both active and passive maps are merged into a new updated active map. 
Considering that the tracks in MIMIR-UW
are designed for loop closure detection, the Atlas map presents an interesting feature to analyze.
On the other hand, the presence of scattering and low-texture areas in the dataset challenges indirect methods like ORB-SLAM, which relies on feature tracking. A family of methods that is of interest for such conditions is direct-based SLAM. Direct-based SLAM optimizes the aggregated photometric error around a parameter that can either be the camera transform or the inverse depth map. %However, due to the high  non-convexity of the photometric error, the minimisation can get easily stuck at local minima, making direct-based methods more prone to fail under large baselines. 
Thus, the direct-based SLAM algorithm DSO \cite{engel2017dso} is deployed, which achieves real-time performance by choosing the high-gradient pixels for the photometric error calculation.


% For visual odometry, the network TartanVO \cite{vo:wang2020tartanvo} is implemented. 
% TartanVO \cite{vo:wang2020tartanvo} implements PWC-Net for optical flow and a modified ResNet50 for pose retrieval, with two output heads for rotation and translation. Aditionally, it inputs the intrinsic camera parameters into a layer between the optical flow and pose retrieval modules to achieve generalization across different cameras. It The loss function is a combination of optical flow and camera motion, with the motion being carried out by an up-to-scale loss function for translation.


The drift between estimated and ground truth trajectories over the keyframe camera poses is evaluated with the \ac{APE} and the \ac{RPE} \cite{sturm2012tumrgbd}. The \ac{APE} and the  \ac{RPE} quantify the global and local consistency of the trajectory, respectively. The trajectories are scaled and aligned for the monocular setups using the least-squares alignment in \cite{grupp2017evo}. For each track, ten tests are carried out, from which the median value is depicted in Table \ref{mimir:table:comparisonSLAM}. 

MIMIR-UW comprises very challenging sequences that lead to tracking failure. Consequently, the success rate (SR) is indicated for DSO. The SR is obtained as the number of succeeded tracks completed out of the ten tests carried out in total.
Because ORB-SLAM was unable to complete the sequences, the mean duration for each retrieved estimate is provided in this case. 
DSO provides better performance in those sequences that is able to complete.
%DSO can be considered to provide better performance than ORB-SLAM, given that is able to complete some of the trajectories with lower errors. 
Higher-textured sequences with higher delentropy values such as SeaFloor and SeaFloor Algae provide lower errors than those with lower texture, like OceanFloor.
Although direct SLAM approaches perform better in low-textured areas, that is not the case in dark sequences. Dark sequences provide visibility of nearby objects solely, which produces higher relative motions and, thus, higher parallax, the main cause of failure for direct SLAM. While ORB-SLAM is less affected by parallax, the lack of features in the image makes tracking fail more often. The stereo setup is less prone to tracking loss, but the error is unexpectedly higher. The repeatability of the environment has been identified as the source cause: both monocular and stereo detect incorrect loop closures within the Atlas map, but with a higher frequency in stereo. 
Consequently, the results show that the current dataset targets the weaknesses and strengths of each SLAM approach to a more significant extent than other existing datasets, opening room for new future lines of development.
%ground truth and estimate coordinate frames are first aligned using Horn's least-squares method \cite{metrics:horn1987closed} to retrieve the transform $S \in Sim_3$ that best aligns $P_i$ with $Q_i$. Then, we measure the absolute euclidean distance between them as:
%\begin{equation}
%    ATE= \sqrt{\frac{1}{n} \sum^n_{i=1}||trans(Q_i^{-1}SP_i)||^2} 
%\end{equation}

% The \ac{RPE} on the other hand quantifies the local consistency of the trajectory. It measures the euclidean distance at time step $i$ between consecutive poses over a fixed time interval $\Delta$, such  that:
% \begin{equation}
%     E_i = \left(Q_i^{-1}Q_{i+\Delta} \right)^{-1}\left(P_i^{-1}P_{i+\Delta} \right)
% \end{equation}
% For the overall trajectory, the \ac{RPE} is obtained as the average $E_i$ over all time intervals $\Delta$:
% \begin{align}
%     RPE^{\Delta} = \sqrt{\frac{1}{m} \sum^m_{i=1}||E_i||^2}  ,
% \end{align}


% with $m = n-\Delta$. %Besides, the \ac{RPE} is computed for both translational and rotational components.
% \begin{figure}[b!]
% \centering

%     \includegraphics[width=.95\linewidth,keepaspectratio]{figures/results/segmentation/Sea floor 2 tracks.png}

%     \includegraphics[width=.95\linewidth,keepaspectratio]{figures/results/segmentation/sand pipe dark.png}


% \caption{}

% \end{figure}

\subsection{Segmentation}

The semantic segmentation labels provided by MIMIR-UW have been used for training and benchmarking two networks for pipeline segmentation. The networks chosen for that purpose are DeepLabV3 \cite{https://doi.org/10.48550/arxiv.1706.0558} and \ac{FCN} \cite{long2015fully}. 
\Ac{FCN} replaces the fully connected layers from classification networks with convolutional layers and upsamples the output to recover the input's dimension. This allows fine-tuning of pre-trained classification networks for segmentation. DeepLabV3 addresses the spatial information loss caused by convolutional layers by implementing dilated convolutions, which support increasing the capture of semantic context without losing spatial resolution.

%replaces the fully connected layers from classification networks with upsampling convolutional layers. This way, the classification representation can be fine-tuned and adapted for segmentation.
Both networks were implemented with ResNet50 as the backbone and using ImageNet pre-trained weights. The network is fine-tuned using an Adam optimizer at a 0.0001 learning rate and cross-entropy loss. The models are trained for a maximum of 50 epochs with early stopping in case of no increase in the validation \ac{mIoU} over six consecutive epochs.
% good mean IOU indicates a balanced performance of precision and recall, mean
%accuracy is an indicator of the detection rate of each class
In MIMIR-UW, pipes account for approximately 10\% of all pixels in all images. For the training sets, each class is weighted inversely proportional to the number of pixels it occupies in the images. To avoid data redundancy, every fifth image is selected in each track.
The segmentation metrics implemented comprise \ac{mIoU}, and the pixel mean accuracy \cite{pascalvocchallenge}. They evaluate the overlap between ground truth and prediction, and the ratio of correctly classified pixels, respectively.
% :
% \begin{align}
%      \text{mean accuracy}&= \frac{1}{C} \sum_{i} p_{ii}/N{i} \\
%     \text{mIoU} &= \frac{1}{C} \sum_{i} (p_{ii})/(M{i}+N{i}-p_{ii}) 
% \end{align}
% With $C$ the number of different classes, $N_i$ the correct number of pixels belonging to class $i$, $M_i$ the number of pixels predicted as class $i$, and $p_{ii}$ the count of correctly classified pixels belonging to class $i$.



%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{table}[hbtp]
\centering
\footnotesize
\caption[Experiment setups and results in the test set for the segmentation networks trained with MIMIR-UW]{Experiment setups and results in the test set for the segmentation networks trained with MIMIR-UW.}
\label{mimir:table:segmentationExperiments}

\begin{tabular}{ l@{\hspace{2mm}} l@{\hspace{2mm}} l@{\hspace{2mm}}   c@{\hspace{1mm}} c@{\hspace{2mm}} c@{\hspace{1mm}} c }

\toprule
%  &\multicolumn{2}{c}{Model setup}  & \multicolumn{5}{c}{Model testing}  \\
% \cmidrule(l{2em}r{2em}){2-3} \cmidrule(l{2em}r{2em}){4-8}
 \multirow{2}{*}[-1.2em]{Experiment}& \multirow{2}{*}[-1.2em]{Data split}& \multirow{2}{*}[-1.2em]{(\%) sequence\{track\}\{camera\}}  &\multicolumn{2}{c}{FCN} & \multicolumn{2}{c}{DeepLabV3}\\ 
\cmidrule(l{.5em}r{1em}){4-5}
\cmidrule(l{.5em}r{1em}){6-7}
%\cline{3-18}
                          &  &  &  mAcc. & mIoU        & mAcc. &  mIoU  \\
\midrule
% SeaFloor 3/3/3          & 70\% SeaFloor\{all-cam0\} & 15\% SeaFloor\{all-cam0\}      & 15\% SeaFloor\{all-cam0\} &   0.916   & \textbf{0.804}        & \textbf{0.920}  &  0.802  \\
% \hline
\multirow{2}{*}{SeaFloor} & Train/Val          & (80/20)\% SeaFloor\{0,1\}\{cam0\}&   \multirow{2}{*}{0.876}   & \multirow{2}{*}{0.785} &  \multirow{2}{*}{\textbf{0.880}} & \multirow{2}{*}{\textbf{0.790}} \\
                          & Test           & 100\% SeaFloor\{2\}\{cam0\}&                             &                        &                                  &  \\
  % \midrule
  
\midrule

% SandPipe light             & 80\% Sandpipe\{light-cam0\} & 20\% Sandpipe\{light-cam0\}      & 100\% Sandpipe\{dark-cam0\} &   0.580   & 0.529        & \textbf{0.644}    & \textbf{0.566}  \\
                                
% \hline
\multirow{2}{*}{SandPipe} & Train/Val               & (80/20)\% Sandpipe\{dark\}\{cam0\}& \multirow{2}{*}{0.643 }   & \multirow{2}{*}{0.445 } &  \multirow{2}{*}{\textbf{0.726}}   & \multirow{2}{*}{\textbf{0.522}}  \\
                          & Test                & 100\% Sandpipe\{light\}\{cam0\}&                         &                         &                                      &  \\
                                

\midrule

\multirow{2}{*}{All} & Train/Val      & (80/20)\% SandPipe,SeaFloor,OceanFloor\{all\}\{cam0\}&  \multirow{2}{*}{\textbf{0.838}}    & \multirow{2}{*}{0.700}   & \multirow{2}{*}{\textbf{0.830}}  & \multirow{2}{*}{\textbf{0.704}}   \\
                     % & Validation   & 20\% SandPipe,SeaFloor,OceanFloor\{all\}\{cam0\}&                                     &                          &                                   &     \\
                     & Test         & 100\% SeaFLoor\_Algae\{all\}\{cam0\}&                                                 &                          &                                     &     \\

\midrule

% \hline
\multirow{2}{.12\linewidth}{S2R - SandPipe} & Train/Val       & (80/20)\% Sandpipe\{light\}\{cam2\}&  \multirow{2}{*}{\textbf{0.552} }    & \multirow{2}{*}{\textbf{0.413}} & \multirow{2}{*}{0.527}  & \multirow{2}{*}{0.380}   \\
                                % & Validation   & 20\% Sandpipe\{light\}\{cam2\}&                                     &                                 &                          & \    \\
                                & Test         &  Real Pipeline Dataset&                                             &                                  &                          &   \\


\midrule

% S2R - Aug. SandPipe   & 80\% Sandpipe\{light\}\{cam2\}             & 20\% Sandpipe\{light\}\{cam2\}     &      Real Pipeline Dataset   &  0.643    & 0.523        & \textbf{0.725}  & \textbf{0.603}   \\

\multirow{2}{.12\linewidth}{S2R - Aug. SandPipe} & Train/Val  & (80/20)\% Sandpipe\{light\}\{cam2\}     &  \multirow{2}{*}{0.712} & \multirow{2}{*}{0.601}  & \multirow{2}{*}{\textbf{0.722}}  & \multirow{2}{*}{ \textbf{0.616}}   \\
                                          % & Validation  & 20\% Sandpipe\{light\}\{cam2\}&                         &                         &                                  &    \\
                                          & Test  &  Real Pipeline Dataset              &                         &                         &                                  &   \\


\midrule

\multirow{2}{.12\linewidth}{S2R - Aug. SeaFloor} & Train/Val   & (80/20)\% SeaFloor\{light\}\{cam2\}      &  \multirow{2}{*}{0.577} & \multirow{2}{*}{0.440 } & \multirow{2}{*}{\textbf{0.583}}  & \multirow{2}{*}{\textbf{0.448}}   \\
                                          % & Validation   & 20\% SeaFloor\{light\}\{cam2\} &                         &                         &                                  &   \\
                                        & Test   &Real Pipeline Dataset                   &                         &                         &                                   &     \\
\bottomrule

\end{tabular}
\end{table}

Six experiments have been carried out as depicted in Table \ref{mimir:table:segmentationExperiments}: three with the data contained by the dataset, and three with data from a real pipeline inspection scenario. Considering that generalization ability is one of the open problems in deep learning, we aim to demonstrate the challenges and opportunities for pipeline segmentation that MIMIR-UW proposes when using state-of-the-art segmentation networks.
The model in the \textit{SeaFloor} experiment has been trained with tracks from the SeaFloor environment exclusively, and tested against an unseen track from the same environment. Despite belonging to different tracks, the proximity of the data distribution yields good results for both segmentation networks.
Similarly, the \textit{SandPipe} experiment used tracks from the SandPipe environment, trained under the dark sequence, and tested in the light one. Despite the similarity in data distribution, the model's performance is inferior to that of \textit{SeaFloor}. One possible reason is how the different lighting conditions affect the object's appearance in the image, yielding a different data distribution for the same pipe model. Some sample results of these experiments can be seen in Fig. \ref{mimir:fig:segmentation}

\begin{figure}[hbtp]
\centering
\begin{tabular}{p{0mm}c@{\hspace{.2mm}}c@{\hspace{.3mm}}c@{\hspace{.2mm}}c@{\hspace{.2mm}}}

& \scriptsize Original image & \scriptsize Ground truth & \scriptsize FCN &  \scriptsize DeepLabV3 \\  
\rotatebox[origin=c]{90}{\scriptsize SeaFloor} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/221-test_1_original.jpg}} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/221-test_1_target.jpg}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/221-test_1_result_FCN.jpg}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/221-test_1_result_DeepLab.jpg}}
\\
%%%%%%%%%%%%%%%%%%%%%
\rotatebox[origin=c]{90}{\scriptsize All} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/All-test_2_original.jpg}} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/All-test_2_target.jpg}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/All-test_2_result_FCN.jpg}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/All-test_2_result_DeepLab.jpg}}
 \\
%%%%%%%%%%%%%%%%%%%%%%5
\rotatebox[origin=c]{90}{\scriptsize S2R} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/original-good10.jpg}} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/GT-good-10.png}} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/FCN-prediction-good-10.jpg}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/DeepLab-prediction-good-10.jpg}} 
\\
%%%%%%%%%%%%%%%%%%%%%%%%
% \rotatebox[origin=c]{90}{\scriptsize S2R} 
% & \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/img-2.png}}
% & \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/GT2.png}} 
% & \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/DeepLab-prediction2.png}} 
% & \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/segmentation/FCN-prediction2.png}}\\

\end{tabular}
\caption[Sample segmentation results for the models from the proposed experiments]{Sample segmentation results for the models trained in the experiments \textit{SeaFloor}, \textit{All}, and \textit{S2R} trained with augmented SandPipe. } %The more favourable lighting conditions in SeaFloor yield better results than SandPipe's dark environment
\label{mimir:fig:segmentation}
\end{figure}


The last experiment for the simulated data involved all the tracks in the dataset, and it is referred to as \textit{All} in Table \ref{mimir:table:segmentationExperiments}.
Here, SeaFloor Algae is only used for testing, while all other environments are used for training and validation. The algae covering the pipes presents an unseen element during training which challenges the model's performance. Nevertheless, the results are close to those in the \textit{SeaFloor} experiment, showing the potential to provide high variability in the training data distribution.

% We aimed to evaluate the model's performance under a wide variety of data. SeaFloor Algae contains pipes partially covered by algae, which
% hinders the models' performance when compared with SeaFloor experiments, as evidenced by the results.
\textit{Sim-to-real} (S2R) comprises a set of experiments for demonstrating the potential of MIMIR-UW in sim-to-real transfer using the real pipeline inspection dataset as the test set.
Fig. \ref{mimir:fig:S2R} showcases the differences in the data between the real and the simulated scenarios. Considering that SandPipe provides closer similarity in terms of colour, shape, and disposition for the pipes in the image, the first \textit{S2R} test is carried out with that dataset as shown in Table \ref{mimir:table:segmentationExperiments}. As is to be expected given the difference in the data distribution, the results show much lower performance than that of the experiment performed purely with simulated data. To approximate more closely the appearance of the simulated pipes to the real ones, the next experiment augments the train and validation splits of the SandPipe dataset by performing random flips and random changes in brightness, contrast and saturation. The images are also converted to grayscale and randomly resized. %The resizing was performed by a factor of three or two, with a 25\% probability in each case. 
As shown in Table \ref{mimir:table:segmentationExperiments}, increasing the variability of the pipeline appearance significantly improves the models' results. The final \textit{S2R} experiment is carried out with the SeaFloor dataset, applying the same augmentation techniques from the previous experiment. It shows how despite the augmentation strategies, a high difference between the  training and test data distribution negatively affects the model's performance.




Fig. \ref{mimir:fig:segmentation} provides a sample of the results. DeepLabV3's ability to encode spatial information allows it to outperform FCN in almost all scenarios. The experiments highlight the potential of MIMIR-UW to explore open challenges in segmentation, such as generalization across environments, sim-to-real transfer and performance under low-light (and thus low delentropy) environments like SandPipe.

% Overall, DeepLabV3's ability to encode spatial information allows it to outperform FCN in almost all scenarios, as exemplified in Fig. \ref{mimir:fig:segmentation}. In addition, the experiments show the potential of using MIMIR-UW to train models to be used in real pipeline inspection scenarios, considering the network's ability to generalize.



\subsection{Depth estimation}

The pertinence of MIMIR-UW for underwater depth estimation is demonstrated by evaluating the performance of two learning-based depth estimation approaches, Monodepth2 \cite{monodepth2}, and MonoRec \cite{wimbauer2020monorec}, on the proposed dataset and the real-world pipeline dataset.
Monodepth2 employs a U-net architecture \cite{ronneberger2015u} with multi-scale depth estimation. This network takes as input a single RGB image.  It predicts a mean normalized inverse depth image by optimizing the per-pixel minimum reprojection error and masking out pixels that do not follow photometric consistency. MonoRec is a framework that consists of 
a cost volume module that encodes geometric information, a mask module that predicts a photometric inconsistency mask, and a depth module that predicts an inverse depth image. The mask and depth modules both employ neural networks that follow a U-net architecture \cite{ronneberger2015u}. This framework takes as input an arbitrary number of RGB images that share a view frustum, the pose transformations between them, and camera intrinsics generated by a structure-from-motion pipeline. MonoRec training consists of four stages, of which only the first stage is considered in this section for evaluation purposes.

\begin{table}[t]
\centering
\footnotesize
\caption[Depth estimation experiments on MIMIR-UW]{Depth estimation experiments on MIMIR-UW.}
\begin{tabular}{ c@{\hspace{2mm}}  c c@{\hspace{2mm}}c  c@{\hspace{2mm}}c  c@{\hspace{2mm}}c  c@{\hspace{2mm}}c }
\toprule
 \multicolumn{2}{r}{ } & \multicolumn{4}{c}{MonoRec} & \multicolumn{4}{c}{MonoDepth2}\\ 
 \cmidrule(l{1.5em}r{1.5em}){3-6} \cmidrule(l{1.5em}r{1.5em}){7-10}
\multicolumn{2}{r}{ } & \multicolumn{2}{c}{KITTI} & \multicolumn{2}{c}{MIMIR} & \multicolumn{2}{c}{KITTI} & \multicolumn{2}{c}{MIMIR}\\
 \cmidrule(l{1.5em}r{1.5em}){3-4} \cmidrule(l{1.5em}r{1.5em}){5-6}
  \cmidrule(l{1.5em}r{1.5em}){7-8} \cmidrule(l{1.5em}r{1.5em}){9-10}
Environment & Track & $SCInv$ & $Abs\text{ }Rel$  & $SCInv$ & $Abs\text{ }Rel$ &  $SCInv$ & $Abs\text{ }Rel$ & $SCInv$ & $Abs\text{ }Rel$    \\
\midrule
\multirow{3}{4em}{SeaFloor} 
                                &  0 & 1.076 & 2.759 & - & - & 0.998 & 0.994 & - & -\\
                                &  1 & 1.054 & 2.341 & - & - & 0.786 & 0.993 & - & -\\
                                &  2 & 1.160 & 1.792 & \textbf{0.7200} & \textbf{0.3390} & 1.094 & 0.996 & \textbf{0.7198} & \textbf{0.4268}\\
\midrule
\multirow{3}{4em}{SeaFloor w. Algae}
                                &  0 & 1.224 & 3.071 & \textbf{1.027} & \textbf{0.7697} & 1.231 & 0.999 & \textbf{0.9612} & \textbf{0.7083}\\
                                &  1 & \textbf{1.027} & 2.202 & 1.112 & \textbf{0.5147} & 1.070 & 0.997 & \textbf{0.8889} & \textbf{0.5235}\\
                                &  2 & 1.177 & 1.929 & \textbf{1.034} & \textbf{0.5657} & 1.179 & 0.995 & \textbf{0.9705} & \textbf{0.6040}\\
\midrule
Real
                        & Pipeline & - & 1.843 & - & \textbf{0.2569} & - & 0.9455 & - & \textbf{0.3977}\\
\bottomrule
\end{tabular}
\label{mimir:table:depthEstimation}
\end{table}
 The networks are trained on a subset of MIMIR-UW and KITTI\cite{dataset:kitti}, to portray the difference between training on underwater and overwater data. The models trained on KITTI are publicly provided by the corresponding authors \cite{monodepth2,wimbauer2020monorec}. In the case of Monodepth, the model trained using monocular and stereo data on an image resolution of $640\times192$ is used. The minimum and maximum depth used for mean normalization of the inverse depth prediction are set to 0.003m and 80m respectively. The MonoRec model used was the first stage depth bootstrap training, trained on an image resolution of $512\times256$.

The subset chosen for MIMIR-UW training involves data from the two front cameras of track 0 and track 1 from the SeaFloor environment, consisting of a total of 9760 images. The training procedures for Monodepth and MonoRec first stage are adhered to, barring a few exceptions. Monodepth is trained on an image resolution of $640\times192$, with a batch size of 32 for 30 epochs, using a pre-trained encoder trained on KITTI. MonoRec is trained on an image resolution of $512\times384$, with a batch size of 16 for 15 epochs, using pretrained weights from first stage training on KITTI.

The evaluation procedure for both networks is different due to a contrast in inference approach. Monodepth predicts a mean normalized inverse depth image, which is then scaled using the median ratio difference between the predicted inverse depth image and the ground truth inverse depth set it is evaluated on, as specified by the authors\cite{monodepth2}. A monocular set of three time-series RGB images, their corresponding ground truth pose transformations, and the camera intrinsics are fed as input to the MonoRec for evaluation. Both approaches are evaluated on the corresponding image resolutions used for training. The evaluation subset chosen from MIMIR-UW involves SeaFloor's track 2, and all tracks from SeaFloor Algae. Additionally, the pseudo ground truth depth for 100 images in the real pipeline dataset is used for evaluation. % and are produced using the photogrammetry framework AliceVision\cite{alicevision2021}.

\begin{figure}[hbtp]
\centering
\begin{tabular}{p{0mm}c@{\hspace{.2mm}}c@{\hspace{.3mm}}c@{\hspace{.2mm}}c@{\hspace{.2mm}}}

& \scriptsize Original image & \scriptsize Ground truth & \scriptsize Monorec &  \scriptsize MonoDepth2 \\  
\rotatebox[origin=c]{90}{\scriptsize SeaFloor}  &\adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/depth/SeafloorTrack2_Img.PNG}} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/depth/SeafloorTrack2_GtDepth.jpg}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/depth/Seafloor_MonorecMimir.jpg}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth]{figs/paper2-mimir/results/depth/Seafloor_MonodepthMimir.jpg}}\\
\rotatebox[origin=c]{90}{\scriptsize Real}  &\adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth, height=.13\linewidth]{figs/paper2-mimir/results/depth/realRGB.jpg}} 
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth, height=.13\linewidth]{figs/paper2-mimir/results/depth/realGTdepth.png}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth, height=.13\linewidth]{figs/paper2-mimir/results/depth/RealMonorecMIMIR.jpg}}
& \adjustbox{valign=m,vspace=.1mm}{\includegraphics[width=.2\linewidth, height=.13\linewidth]{figs/paper2-mimir/results/depth/realMonodepthMIMIR.jpg}}\\


\end{tabular}
\caption[Sample results for the depth estimation experiments]{Sample results on the SeaFloor track and the real pipe for the depth estimation models trained on MIMIR-UW. } 
\label{mimir:fig:depth}
\end{figure}

The error between ground truth and predicted inverse depth is quantified with the standard metrics scale-invariant error ($SCInv$) and absolute relative distance ($Abs\text{ }Rel$) \cite{depthMetricscaleeigensplit}, yielding the results shown in Table \ref{mimir:table:depthEstimation}. 
% We quantify the error between ground truth and predicted inverse depth images using the standard metrics, scale-invariant error ($SCInv$) and absolute relative distance ($Abs\text{ }Rel$) \cite{depthMetricscale}. 
% We quantify the error between $n$ pixels in the predicted inverse depth image $\hat{D_i}$ and $n$ pixels in the ground truth inverse depth image $D_i$ using the scale-invariant error ($SCInv$) and the absolute relative distance ($Abs\text{ }Rel$). 
% %Absolute differences between depths typically contain scale difference as a significant fraction of the error. The exponential of log differences of the depth will be the same for all scalar multiples of the depth, thus exhibiting scale-invariance.
% The absolute relative distance ($Abs\text{ }Rel$) is a calculation of the normalized absolute L1 distance between $\frac{1}{\hat{D_i}}$ and $\frac{1}{D_i}$ with the ground truth depth $\frac{1}{D_i}$ being the normalizing factor. The formulation is presented in (\ref{absreldepth}) for completeness as a standard metric for depth estimation:
% \begin{equation}
% \label{absreldepth}
%    Abs\text{ }Rel = \frac{1}{n}\sum^{n}_{i=0}\frac{\big\lvert\frac{1}{\hat{D_i}} - \frac{1}{D_i}\big\rvert}{\frac{1}{D_i}} 
% \end{equation}
% The motivation for scale-invariance is a consequence of the global scale ambiguity of depth predictions.  The scale-invariant error ($SCInv$) is chosen to mitigate this problem by taking the mean-squared error between $\hat{D_i}$ and $D_i$ in logarithmic space \cite{depthMetricscale}.  The error can be formulated as follows:
% \begin{align}
%     E &= log(\hat{D_i}) - log(D_i) \nonumber \\ \nonumber \\
%    SCInv &= \sqrt{\frac{1}{n}\sum^{n}_{i=1}E^2 -\frac{1}{n^2}\bigg{(}\sum^{n}_{i=1}E\bigg{)}^2} \label{scaleinvarianterr}
% \end{align}
% The evaluation results are shown in Table \ref{mimir:table:depthEstimation}. 
Comparing performance between both networks would be unfair, because Monodepth uses the ground truth depth from evaluation data to influence its result. A comparison between training datasets is considered instead, to validate the importance of MIMIR-UW. The networks trained on MIMIR-UW outperform KITTI training across all of the evaluation data chosen. For Monorec, training on MIMIR-UW yields a lower error with both metrics, on all tracks except track 1 of SeaFloor with algae where the $SCInv$ was slightly higher. This can be attributed to the difficulty of measuring depth for deformable objects like algae, and is confirmed by the difference in $Abs Rel$ errors for MIMIR-UW training between SeaFloor and SeaFloor with Algae. The same results can be concluded from training on Monodepth, where MIMIR-UW outperformed KITTI on all metrics. When trained on SeaFloor data, both networks exhibit a lower $Abs Rel$ on real data compared to simulation, indicating that the simulation may possess difficulties for the depth estimation task not present in real data. However, a sim-to-real transfer cannot be validated using this evaluation on real data, due to the high level of sparsity of ground truth per depth image (see Fig. \ref{mimir:fig:depth}). Large evaluation metric errors of networks trained on KITTI dataset indicate that neural networks trained on datasets above water cannot generalize well to difficult underwater imaging conditions. Alternately, training on synthetic underwater data in the proposed dataset yields better performance on simulation and real-word data, thus indicating the need for MIMIR-UW.
%The results of the evaluation are shown in Table \ref{mimir:table:depthEstimation}. Overall, both approaches yield a high error level on the scale-invariant metric and absolute relative distance. It can be seen that Monodepth2 performs better than MonoRec on both metrics, except for tracks in the Seafloor Algae environment. The overall performance difference could be attributed to the properties of the mask module in MonoRec, which is trained on photometric inconsistency in the KITTI dataset. This additional supervised training could lead to further overfitting, suggesting MonoRec is more incapable of generalization than Monodepth2, despite the inclusion of geometric information as a prior. Evaluation of both approaches indicate that neural networks trained on datasets above water cannot generalize well to difficult underwater imaging conditions, thus indicating the need for MIMIR-UW. 




