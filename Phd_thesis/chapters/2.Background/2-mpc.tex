% Assuming Chapter/Section numbering continues from previous context
\section{Overview of Model Predictive Control}

Model Predictive Control (MPC) is an advanced control strategy that leverages a dynamic model of the system to predict its future behavior and optimize control actions over a finite time period, known as the prediction horizon ($T_p$). At each control cycle, MPC solves an online optimization problem to find a sequence of future control inputs that minimize a predefined cost function, subject to constraints on the system's states and inputs.

A key characteristic of MPC is its receding horizon approach. Although an entire sequence of future control inputs is computed, only the first input in the sequence is applied to the system over the next sampling interval ($\delta$). At the subsequent time step, the system's state is measured or estimated, and the prediction horizon shifts forward. The optimization problem is then solved again with this updated information, providing feedback and adapting to changing conditions.

For underwater vehicles, MPC offers significant benefits:
\begin{itemize}
    \item \textbf{Constraint Handling:} Systematically incorporates operational limits, such as maximum thruster forces (represented as limits on generalized forces $\boldsymbo{\tau}$), velocity restrictions, or safe operating areas, directly into the control problem formulation.
    \item \textbf{Predictive Capability:} Anticipates future system behavior, allowing proactive control actions in response to reference trajectory changes or predictable disturbances.
    \item \textbf{Optimality:} Balances competing objectives, such as precise trajectory tracking versus minimizing control effort, through the design of the cost function.
    \item \textbf{Nonlinearity Handling:} Can explicitly utilize the nonlinear dynamic models (as derived previously, e.g., in Section \ref{sec}), capturing complex hydrodynamic effects more accurately than linear controllers.
\end{itemize}


\subsection{The MPC Algorithm Formulation}
The core MPC algorithm involves repeatedly solving an optimal control problem. Let the system state be $\boldsymbol{x} = [\boldsymbol{\eta}^T, \boldsymbol{\nu}^T]^T \in \mathbb{R}^{12}$ (combining position/orientation and velocities), and the control input be the generalized force/moment vector $\boldsymbol{\tau} \in \mathbb{R}^6$. The continuous-time dynamics are $\dot{\boldsymbol{x}} = f(\boldsymbol{x}, \boldsymbol{\tau})$, representing the vehicle's equations of motion.

The optimization problem solved at time $t$ is typically formulated as:
\begin{mini!}|s|
{\substack{\boldsymbol{\bar{\tau}}(\cdot) \\ \text{over } [t, t+T_p]}}
{J = \int_t^{t+T_p} L(\boldsymbol{\bar{x}}(\sigma), \boldsymbol{\bar{\tau}}(\sigma)) \, d\sigma + E(\boldsymbol{\bar{x}}(t+T_p))}
{\label{eq:mpc_opt}}
{}
\addConstraint{\dot{\boldsymbol{\bar{x}}}(\sigma)}{= f(\boldsymbol{\bar{x}}(\sigma), \boldsymbol{\bar{\tau}}(\sigma))}{\quad \forall \sigma \in [t, t+T_p]}
\addConstraint{\boldsymbol{\bar{x}}(t)}{= \boldsymbol{x}(t)}{}
\addConstraint{\boldsymbol{\bar{\tau}}(\sigma)}{\in \mathbb{T}}{\quad \forall \sigma \in [t, t+T_p]}
\addConstraint{\boldsymbol{\bar{x}}(\sigma)}{\in \mathbb{X}}{\quad \forall \sigma \in [t, t+T_p]}
\end{mini!}

In this formulation, $\boldsymbol{\bar{x}}(\sigma)$ and $\boldsymbol{\bar{\tau}}(\sigma)$ represent the predicted state and control trajectories over the prediction horizon $[t, t+T_p]$, while $\boldsymbol{x}(t)$ denotes the current measured or estimated state. The stage cost $L(\cdot, \cdot)$ is typically defined as a quadratic function $L(\boldsymbol{x}, \boldsymbol{\tau}) = \|\boldsymbol{x} - \boldsymbol{x}_{ref}\|_{\boldsymbol{Q}}^2 + \|\boldsymbol{\tau}\|_{\boldsymbol{R}}^2$, penalizing deviations from a reference state $\boldsymbol{x}_{ref}$ and control effort, weighted by matrices $\boldsymbol{Q} \succeq 0$ and $\boldsymbol{R} \succ 0$. The terminal cost $E(\cdot)$ is often expressed as $E(\boldsymbol{x}) = \|\boldsymbol{x} - \boldsymbol{x}_{ref}(t+T_p)\|_{\boldsymbol{P}}^2$, with $\boldsymbol{P} \succeq 0$ selected based on stability considerations. The sets $\mathbb{T}$ and $\mathbb{X}$ define the admissible control inputs and states, respectively.

The MPC control loop is summarized in Algorithm \ref{alg:mpc_loop}.
\begin{algorithm}[H]
\caption{MPC Control Loop}
\label{alg:mpc_loop}
\begin{algorithmic}[1]
    \State At time $t$, measure or estimate the current state $\boldsymbol{x}(t)$.
    \State Solve the optimal control problem \eqref{eq:mpc_opt} to obtain the optimal control sequence $\boldsymbol{\bar{\tau}}^*(\sigma)$ for $\sigma \in [t, t+T_p]$.
    \State Apply only the first part of the optimal control input, $\boldsymbol{\tau}_{MPC}(t) = \boldsymbol{\bar{\tau}}^*(t)$, to the system (or as input to a lower-level control allocation if used) over the interval $[t, t+\delta)$.
    \State Wait until the next sampling time $t+\delta$. Set $t \leftarrow t+\delta$ and go back to Step 1.
\end{algorithmic}
\end{algorithm}





\subsection{Discretization for Numerical Solution}
The continuous-time optimal control problem \eqref{eq:mpc_opt} must be discretized to be solved numerically. The prediction horizon $T_p$ is divided into $N$ intervals of length $\Delta t = T_p/N$. A common approach is to assume piecewise constant control inputs over each interval:
\begin{equation}
    \boldsymbo{\tau}(t) = \boldsymbo{\tau}_k \quad \text{for} \quad t \in [t_k, t_{k+1}), \quad k = 0, \dots, N-1
\end{equation}
where $t_k = t + k\Delta t$. The continuous state dynamics $\dot{\boldsymbo{x}} = f(\boldsymbo{x}, \boldsymbo{\tau})$ are discretized using a numerical integration method (e.g., Euler, Runge-Kutta) to obtain a discrete-time prediction model:
\begin{equation}
    \boldsymbo{x}_{k+1} = f_d(\boldsymbo{x}_k, \boldsymbo{\tau}_k)
\end{equation}
For instance, using the 4th-order Runge-Kutta (RK4) method:
\begin{equation}
    \boldsymbo{x}_{k+1} = \boldsymbo{x}_k + \frac{\Delta t}{6}(\mat{k}_1 + 2\mat{k}_2 + 2\mat{k}_3 + \mat{k}_4)
\end{equation}
where $\mat{k}_1 = f(\boldsymbo{x}_k, \boldsymbo{\tau}_k)$, $\mat{k}_2 = f(\boldsymbo{x}_k + \frac{\Delta t}{2}\mat{k}_1, \boldsymbo{\tau}_k)$, etc.
The cost function integral is approximated by a sum:
\begin{equation}
    J_d = \sum_{k=0}^{N-1} L_d(\boldsymbo{x}_k, \boldsymbo{\tau}_k) + E(\boldsymbo{x}_N)
\end{equation}
where $L_d(\boldsymbo{x}_k, \boldsymbo{\tau}_k) \approx \int_{t_k}^{t_{k+1}} L(\boldsymbo{\bar{x}}(\sigma), \boldsymbo{\tau}_k) \, d\sigma$, often simplified to $L_d = \Delta t \left( \|\boldsymbo{x}_k - \boldsymbo{x}_{ref,k}\|_{\mat{Q}}^2 + \|\boldsymbo{\tau}_k\|_{\mat{R}}^2 \right)$.

\subsection{Solving the Optimization Problem}
Discretization transforms the optimal control problem into a Nonlinear Program (NLP). The decision variables are the sequence of control inputs $\boldsymbo{T}_U = \{\boldsymbo{\tau}_0, \dots, \boldsymbo{\tau}_{N-1}\}$.

\subsubsection{Direct Methods}
Direct methods treat both the control inputs $\boldsymbo{T}_U$ and the resulting state sequence $\boldsymbo{T}_X = \{\boldsymbo{x}_1, \dots, \boldsymbo{x}_N\}$ as optimization variables (with $\boldsymbo{x}_0 = \boldsymbo{x}(t)$ fixed). The discretized dynamics $ \boldsymbo{x}_{k+1} = f_d(\boldsymbo{x}_k, \boldsymbo{\tau}_k) $ are enforced as equality constraints. The NLP takes the form:
\begin{equation}
\begin{aligned}
\min_{\boldsymbo{T}_U, \boldsymbo{T}_X} \quad & \sum_{k=0}^{N-1} L_d(\boldsymbo{x}_k, \boldsymbo{\tau}_k) + E(\boldsymbo{x}_N) \\
\text{s.t.} \quad & \boldsymbo{x}_{k+1} = f_d(\boldsymbo{x}_k, \boldsymbo{\tau}_k), \quad k=0, \dots, N-1 \\
& \boldsymbo{\tau}_k \in \mathbb{T}, \quad k=0, \dots, N-1 \\
& \boldsymbo{x}_k \in \mathbb{X}, \quad k=1, \dots, N \\
& \boldsymbo{x}_0 = \boldsymbo{x}(t)
\end{aligned}
\end{equation}
This structured NLP can be solved using general-purpose NLP solvers.

\subsubsection{Sequential Quadratic Programming (SQP)}
SQP is a common iterative method for solving NLPs. At each iteration $k$, it approximates the NLP by a Quadratic Program (QP) obtained by linearizing the constraints and using a quadratic approximation of the Lagrangian function. The QP subproblem typically looks like:
\begin{equation}
\begin{aligned}
\min_{\Delta\boldsymbo{z}} \quad & \frac{1}{2}\Delta\boldsymbo{z}^T \mat{H}_k \Delta\boldsymbo{z} + \boldsymbo{g}_k^T \Delta\boldsymbo{z} \\
\text{s.t.} \quad & \mat{A}_{eq,k} \Delta\boldsymbo{z} = \boldsymbo{b}_{eq,k} \\
& \mat{A}_{ineq,k} \Delta\boldsymbo{z} \leq \boldsymbo{b}_{ineq,k}
\end{aligned}
\end{equation}
where $\Delta\boldsymbo{z}$ represents the step change in the optimization variables (e.g., $\boldsymbo{T}_U, \boldsymbo{T}_X$), $\mat{H}_k$ approximates the Hessian of the Lagrangian, $\boldsymbo{g}_k$ is the gradient of the objective function, and $\mat{A}_{eq,k}, \mat{A}_{ineq,k}$ are Jacobians of the constraints, all evaluated at the current iterate $\boldsymbo{z}_k$. Solving this QP yields a search direction $\Delta\boldsymbo{z}$, and the next iterate is found using a line search.

% \subsection{Implementation Considerations}
% Implementing MPC effectively involves several practical aspects:

% \subsubsection{Real-Time Computation} % Kept this subsubsection title
% Solving the optimization problem online at each sampling instant $\delta$ requires significant computational power, especially for fast systems or long horizons. Techniques to enable real-time MPC include:
% \begin{itemize}
%     \item \textbf{Efficient Solvers:} Using structure-exploiting optimization algorithms (e.g., interior-point methods, active-set methods tailored for MPC).
%     \item \textbf{Warm-Starting:} Initializing the solver at time $t+\delta$ with the shifted optimal solution from time $t$.
%     \item \textbf{Reducing Complexity:} Using techniques like move blocking (keeping control input constant over several steps) or shortening the prediction horizon $T_p$. A common rule of thumb suggests $T_p$ should cover the dominant dynamics, e.g., 5-10 times the vehicle's main time constant.
%     \item \textbf{Constraint Softening:} Relaxing constraints slightly using penalty terms if the problem becomes infeasible, ensuring the solver always returns a (possibly suboptimal) solution.
% \end{itemize}

% \subsubsection{Other Considerations} % Combined stability mention here
% Beyond computational performance, ensuring the stability of the closed-loop system is a crucial aspect of practical MPC design, particularly when dealing with nonlinear dynamics and constraints. While rigorous stability guarantees often involve specific choices of terminal costs and constraints, these theoretical aspects are complex and depend heavily on the system and controller tuning. For this overview, we acknowledge stability as an important factor to consider during implementation and validation, although detailed analysis methods are not covered here.

% --- Optional: Combine Implementation Considerations into one subsection ---
\subsection{Implementation Considerations}
Implementing MPC effectively on a marine robot involves addressing several practical aspects.

Firstly, \textbf{real-time computation} is critical. Solving the optimization problem online at each sampling instant $\delta$ demands significant computational resources. Common techniques to achieve real-time performance include: using efficient, structure-exploiting optimization solvers; warm-starting the solver at time $t+\delta$ with the shifted solution from time $t$; reducing problem complexity via methods like move blocking (keeping control inputs constant over several steps) or adjusting the prediction horizon $T_p$ (which typically needs to be long enough to capture relevant dynamics); and potentially using constraint softening to handle occasional infeasibility gracefully.

Secondly, while the receding horizon nature provides feedback, \textbf{closed-loop stability} is not automatically guaranteed, especially for nonlinear systems with constraints. Ensuring stability is an important consideration in practical MPC design and tuning. Although formal methods exist (often involving specific choices for the terminal cost $E(\cdot)$ and potentially terminal constraints), they add complexity and are beyond the scope of this introductory overview. Stability performance in practice often needs to be verified through extensive simulation and experimental testing.


% \subsubsection{Control Allocation}
% The MPC controller typically computes the desired generalized forces and moments $\boldsymbo{\tau}_{MPC} \in \mathbb{R}^6$. These must then be distributed among the available actuators (e.g., $n$ thrusters). This is the thruster allocation problem, often solved as a separate, computationally cheaper optimization problem at each step:
% \begin{equation} \label{eq:thruster_alloc}
% \begin{aligned}
% \min_{\boldsymbo{T}} \quad & \|\mat{B}\boldsymbo{T} - \boldsymbo{\tau}_{MPC}\|^2 + \epsilon \|\boldsymbo{T}\|^2 \\ % Added regularization
% \text{s.t.} \quad & \boldsymbo{T}_{min} \leq \boldsymbo{T} \leq \boldsymbo{T}_{max}
% \end{aligned}
% \end{equation}
% where $\boldsymbo{T} \in \mathbb{R}^n$ is the vector of individual thruster commands (denoted $\boldsymbo{u}_{thr}$ previously), $\mat{B}$ is the $6 \times n$ thrust configuration matrix (from Section \ref{sec}), and $\boldsymbo{T}_{min}, \boldsymbo{T}_{max}$ are the saturation limits. A small regularization term ($\epsilon > 0$) can be added for robustness. This is typically a constrained least-squares problem, often solved using fast QP solvers.


