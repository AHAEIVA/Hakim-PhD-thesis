\section{Model Predictive Control}
\label{sec:MPC_background}
MPC is an optimization-based control strategy that leverages a dynamic model of the system to predict its future behavior and optimize control actions over a finite time period, known as the prediction horizon ($T_p$). At each control cycle, MPC solves an online optimization problem to find a sequence of future control inputs that minimize a predefined cost function, subject to constraints on the system's states and inputs.


\subsection{Receding Horizon Optimization}
A key characteristic of MPC is its receding horizon approach. Although an entire sequence of future control inputs is computed, only the first input in the sequence is applied to the system over the next sampling interval ($\delta$), where $\delta$ is the controller update period. At the subsequent time step, the system's state is measured or estimated, and the prediction horizon shifts forward. The optimization problem is then solved again with this updated information, providing feedback and adapting to changing conditions.

The core MPC algorithm involves repeatedly solving an optimal control problem. The optimization problem solved at time $t$ is typically formulated as:

\begin{mini!}|s|
{\mathbf{\bar{u}}(\cdot)}
{J = \int_t^{t+T_p} L(\boldsymbol{\bar{x}}(\tau), \mathbf{\bar{u}}(\tau)) \, d\tau + E(\boldsymbol{\bar{x}}(t+T_p))}
{\label{eq:mpc_opt}}
{}
\addConstraint{\dot{\boldsymbol{\bar{x}}}(\tau)}{= f(\boldsymbol{\bar{x}}(\tau), \mathbf{\bar{u}}(\tau))}{\quad \forall \tau \in [t, t+T_p]}
\addConstraint{\boldsymbol{\bar{x}}(t)}{= \boldsymbol{x}(t)}{}
\addConstraint{\mathbf{\bar{u}}(\tau)}{\in \mathcal{U}}{\quad \forall \tau \in [t, t+T_p]}
\addConstraint{\boldsymbol{\bar{x}}(\tau)}{\in \mathcal{X}}{\quad \forall \tau \in [t, t+T_p]}
\end{mini!}

In this formulation, $\boldsymbol{\bar{x}}(\tau)$ and $\mathbf{\bar{u}}(\tau)$ represent the predicted state and control trajectories over the prediction horizon $[t, t+T_p]$, where $\tau$ is the integration time variable over the interval $[t, t+T_p]$, while $\boldsymbol{x}(t)$ denotes the current measured or estimated state. The stage cost $L(\cdot, \cdot)$ is typically defined as a quadratic function $L(\boldsymbol{x}, \mathbf{u}) = \|\boldsymbol{x} - \boldsymbol{x}_{ref}\|_{\boldsymbol{Q}}^2 + \|\mathbf{u}\|_{\boldsymbol{R}}^2$, penalizing deviations from a reference state $\boldsymbol{x}_{ref}$ and control effort, weighted by matrices $\boldsymbol{Q} \succeq 0$ and $\boldsymbol{R} \succ 0$. The terminal cost $E(\cdot)$ is often expressed as $E(\boldsymbol{x}) = \|\boldsymbol{x} - \boldsymbol{x}_{ref}(t+T_p)\|_{\boldsymbol{P}}^2$, with $\boldsymbol{P} \succeq 0$ selected based on stability considerations. The sets $\mathcal{U}$ and $\mathcal{X}$ define the admissible control inputs and states, respectively.

The MPC control loop is summarized in Algorithm \ref{alg:mpc_loop}.

\begin{algorithm}[H]
\caption{MPC Control Loop}
\label{alg:mpc_loop}
\SetAlgoLined
\KwIn{Current time $t$}
\KwOut{Control input $\mathbf{u}^*(t)$}

\While{MPC is running}{
    Measure or estimate the current state $\boldsymbol{x}(t)$\;
    Solve the optimal control problem~\eqref{eq:mpc_opt} to get optimal control sequence $\mathbf{\bar{u}}^*(\tau)$ for $\tau \in [t, t+T_p]$\;
    Apply the first control input $\mathbf{u}^*(t) = \mathbf{\bar{u}}^*(t)$ over $[t, t+\delta)$\;
    %Wait until the next sampling time $t+\delta$\;
    %Update $t \leftarrow t+\delta$\;
}
\end{algorithm}

\subsection{Discretization for Numerical Solution}

The continuous-time optimal control problem \eqref{eq:mpc_opt} must be discretized to be solved numerically. The prediction horizon $T_p$ is divided into $N$ intervals of length $\Delta t = T_p/N$. A common approach is to assume piecewise constant control inputs over each interval:

\begin{equation}
    \mathbf{u}(t) = \mathbf{u}_k \quad \text{for} \quad t \in [t_k, t_{k+1}), \quad k = 0, \dots, N-1
\end{equation}

where $t_k = t + k\Delta t$. Here, $\boldsymbol{x}_k \equiv \boldsymbol{\bar{x}}(t_k)$ and $\mathbf{u}_k \equiv \mathbf{\bar{u}}(\tau)$ for $\tau \in [t_k, t_{k+1})$ represent the discrete-time equivalents of the continuous predicted trajectories. The continuous state dynamics $\dot{\boldsymbol{x}} = f(\boldsymbol{x}, \mathbf{u})$ are discretized using a numerical integration method (e.g., Euler, Runge-Kutta) to obtain a discrete-time prediction model:

\begin{equation}
    \boldsymbol{x}_{k+1} = f_d(\boldsymbol{x}_k, \mathbf{u}_k)
\end{equation}

For instance, using the 4th-order Runge-Kutta (RK4) method:

\begin{equation}
    \boldsymbol{x}_{k+1} = \boldsymbol{x}_k + \frac{\Delta t}{6}(\mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 + \mathbf{k}_4)
\end{equation}

where $\mathbf{k}_1 = f(\boldsymbol{x}_k, \mathbf{u}_k)$, $\mathbf{k}_2 = f(\boldsymbol{x}_k + \frac{\Delta t}{2}\mathbf{k}_1, \mathbf{u}_k)$, $\mathbf{k}_3 = f(\boldsymbol{x}_k + \frac{\Delta t}{2}\mathbf{k}_2, \mathbf{u}_k)$, and $\mathbf{k}_4 = f(\boldsymbol{x}_k + \Delta t \mathbf{k}_3, \mathbf{u}_k)$.

The cost function integral is approximated by a sum:

\begin{equation}
    J_d = \sum_{k=0}^{N-1} L_d(\boldsymbol{x}_k, \mathbf{u}_k) + E(\boldsymbol{x}_N)
\end{equation}

where $L_d(\boldsymbol{x}_k, \mathbf{u}_k) \approx \int_{t_k}^{t_{k+1}} L(\boldsymbol{\bar{x}}(\tau), \mathbf{\bar{u}}(\tau)) \, d\tau \approx \Delta t \cdot L(\boldsymbol{x}_k, \mathbf{u}_k)$, often simplified to $L_d(\boldsymbol{x}_k, \mathbf{u}_k) = \Delta t \left( \|\boldsymbol{x}_k - \boldsymbol{x}_{ref,k}\|_{\boldsymbol{Q}}^2 + \|\mathbf{u}_k\|_{\boldsymbol{R}}^2 \right)$.

\subsection{Solving the Optimization Problem}

The discretized problem is now a finite-dimensional optimization problem with $6N$ control variables $(\mathbf{u}_0, \ldots, \mathbf{u}_{N-1})$ and potentially $12N$ state variables $(\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N)$, subject to $12N$ nonlinear equality constraints from the dynamics and additional inequality constraints from $\mathcal{U}$ and $\mathcal{X}$. This forms a Nonlinear Program (NLP).


\subsubsection{Direct Methods}

Direct methods treat both the sequence of control inputs $\mathbf{U} = (\mathbf{u}_0, \dots, \mathbf{u}_{N-1})$ and the resulting state sequence $\mathbf{X} = (\boldsymbol{x}_1, \dots, \boldsymbol{x}_N)$ as optimization variables (with $\boldsymbol{x}_0 = \boldsymbol{x}(t)$ fixed). The discretized dynamics $\boldsymbol{x}_{k+1} = f_d(\boldsymbol{x}_k, \mathbf{u}_k)$ are enforced as equality constraints. The NLP takes the form:

\begin{equation}
\begin{aligned}
\min_{\mathbf{U}, \mathbf{X}} \quad & \sum_{k=0}^{N-1} L_d(\boldsymbol{x}_k, \mathbf{u}_k) + E(\boldsymbol{x}_N) \\
\text{s.t.} \quad & \boldsymbol{x}_{k+1} = f_d(\boldsymbol{x}_k, \mathbf{u}_k), \quad k=0, \dots, N-1 \\
& \mathbf{u}_k \in \mathcal{U}, \quad k=0, \dots, N-1 \\
& \boldsymbol{x}_k \in \mathcal{X}, \quad k=1, \dots, N \\
& \boldsymbol{x}_0 = \boldsymbol{x}(t)
\end{aligned}
\end{equation}

%This structured NLP can be solved using general-purpose NLP solvers.

\subsubsection{Sequential Quadratic Programming}

Due to the nonlinear dynamics $f_d(\cdot,\cdot)$ and potentially nonlinear constraints, this NLP is typically solved using iterative methods. Sequential Quadratic Programming (SQP) is particularly well-suited for MPC because it can exploit the problem's structure and provide good convergence properties.

Sequential Quadratic Programming (SQP) is a common iterative method for solving NLPs. At each iteration $j$, it approximates the NLP by a Quadratic Program (QP). To define this QP, let's first define a single concatenated vector, $\boldsymbol{z}$, that contains all of the decision variables. This includes the sequence of control inputs and the sequence of predicted states:
$$
\boldsymbol{z} = [\mathbf{u}_0^T, \dots, \mathbf{u}_{N-1}^T, \boldsymbol{x}_1^T, \dots, \boldsymbol{x}_N^T]^T
$$
The QP subproblem solved at iteration $j$ is then formulated to find the optimal step change, $\Delta\boldsymbol{z}$:

\begin{equation}
\begin{aligned}
\min_{\Delta\boldsymbol{z}} \quad & \frac{1}{2}\Delta\boldsymbol{z}^T \boldsymbol{H}_j \Delta\boldsymbol{z} + \boldsymbol{g}_j^T \Delta\boldsymbol{z} \\
\text{s.t.} \quad & \boldsymbol{A}_{eq,j} \Delta\boldsymbol{z} = \boldsymbol{b}_{eq,j} \\
& \boldsymbol{A}_{ineq,j} \Delta\boldsymbol{z} \leq \boldsymbol{b}_{ineq,j}
\end{aligned}
\end{equation}

where $\Delta\boldsymbol{z}$ represents the step change in the optimization variables, $\boldsymbol{H}_j$ is a positive-definite approximation of the Hessian of the Lagrangian, $\boldsymbol{g}_j$ is the gradient of the objective function, $\boldsymbol{A}_{eq,j}$ and $\boldsymbol{A}_{ineq,j}$ are the Jacobians of the equality and inequality constraints, respectively, and $\boldsymbol{b}_{eq,j}$ and $\boldsymbol{b}_{ineq,j}$ are the linearized constraint violation vectors, all evaluated at the current iterate $\boldsymbol{z}_j$. Solving this QP yields a search direction $\Delta\boldsymbol{z}$, and the next iterate is found using a line search, that is,is, $\boldsymbol{z}_{j+1} = \boldsymbol{z}_j + \alpha \Delta\boldsymbol{z}$, where $\alpha$ is a step length guaranteeing descent \cite{nocedal1999numerical}.


\subsection{Implementation Considerations}

Implementing MPC effectively on a marine robot involves addressing several practical aspects. Firstly, real-time computation is critical. Solving the optimization problem online at each sampling instant $\delta$ demands significant computational resources. Common techniques to achieve real-time performance include: using efficient, structure-exploiting optimization solvers; warm-starting the solver at time $t+\delta$ with the shifted solution from time $t$ \cite{gros2020linear} or adjusting the prediction horizon $T_p$ (which typically needs to be long enough to capture relevant dynamics); and potentially using constraint softening to handle occasional infeasibility gracefully.

Secondly, while the receding horizon nature provides feedback, closed-loop stability is not automatically guaranteed, especially for nonlinear systems with constraints. Ensuring stability is an important consideration in practical MPC design and tuning. Although formal methods exist (often involving specific choices for terminal cost $E(\cdot)$ and potentially terminal constraints \cite{hao2017nonlinear}), they add computational complexity, making it impractical to implement in real time.

In the following chapters, we will demonstrate how MPC can be utilized in conjunction with data-driven model learning and task-specific path planners to establish a robust framework for complex inspection tasks.