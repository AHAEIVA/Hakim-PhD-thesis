\section{SLAM: Front-end}
\label{sec:frontend}
Front-end SLAM comprises the lower abstraction layer of SLAM. In the front-end, the raw sensor inputs are abstracted into a model used for tracking and localization.

\subsection{Tracking}
\label{sec:frontend:tracking}
While the overall objective of SLAM is to estimate a globally consistent camera trajectory within a map, the tracking thread incrementally obtains an estimate of the local trajectory.
The tracking module was first implemented as a separate thread in \ac{PTAM} \cite{klein2007ptam}. This way, \ac{PTAM} achieved better performance by removing the need to update the map for each new frame.
The formulation of the tracking thread in monocular visual SLAM is equivalent to that of \ac{VO} \cite{nister2004visual}: it aims to compute the set of relative rigid body transformations $T_{k-1, k}$ between subsequent image frames. 
The global camera poses $C_{0:k}$ can be obtained by concatenating the relative transforms as:
\begin{equation}
    C_{k} = C_{k-1}T_{k-1, k}
\end{equation}

Depending on how motion is estimated from adjacent image frames, we can classify the methods for visual tracking as direct or indirect.
Direct or appearance-based methods estimate the motion from the intensity information of all pixels in the image. Here, the optimization is based on the photometric error, as shown in Fig. \ref{fig:photometric}. Since these methods use more information from the image, they tend to be more robust under areas with low texture and repetitive features, but at a higher computational cost. However, they are more sensitive to illumination changes and large baselines than indirect methods.
Indirect or feature-based methods deduct the motion from the geometric constraints between salient features detected in both images. This computation is based on the optimization of the reprojection error, as shown in Fig. \ref{fig:reprojection}. While computationally inexpensive, these methods discard much valuable information on the image.


In this subsection, direct and indirect methods for tracking in monocular visual SLAM are presented and compared with formulations in the literature.


\subsubsection{Features and descriptors\label{section:features-descriptors}}
Sparse methods use visual features in the images to estimate the camera pose. The neighbourhood around a feature point is called a patch. When a feature is detected, the shape and structure of its patch are embedded into a metric space called descriptor. The descriptors are matched according to a distance function that will depend on the descriptor space: for binary descriptors, it is the Hamming distance, while for histogram-based descriptors, it could be the L1 or the L2 norm. 
Some desired capabilities for a descriptor are invariance under image transformation, equivariance under scale changes, computational efficiency, repeatability and storage efficiency.

The traditional approaches in computer vision have used handcrafted descriptors. Regarding SLAM, the binary descriptors are the primary choice since they achieve good trade-off between robustness and computational efficiency. Some prevailing descriptors for SLAM are \ac{BRIEF}\cite{calonder2011brief}, \ac{ORB}\cite{rublee2011orb}, and A-KAZE \cite{alcantarilla2011AKAZE}.




The  descriptors for \ac{BRIEF} are computed as binary strings from image patches, making them efficient to compute and store \cite{calonder2011brief}. 
Some SLAM implementations using \ac{BRIEF} features are VINS-Mono \cite{qin2018vins-mono}, VPS-SLAM \cite{bavle2020vps-slam} and OV$^2$ SLAM\cite{ferrera2021ov2slam}. VITAMIN-E \cite{Yokozuka_2019vitamine} uses the BRIEF feature without computing the descriptor.
The \ac{ORB} is based on FAST for the features and BRIEF for the descriptors. \ac{ORB} features are computationally efficient and rotation-invariant, and they also achieve robustness in scale by using a scale pyramid of the image \cite{rublee2011orb}. ORB-SLAM \cite{mur2015orb} uses \ac{ORB} features for mapping, tracking, and loop detection. 


Handcrafted features are more prone to outliers in the matching than learned features, negatively affecting the tracking precision. Deep learning-based features are richer and thus easier to match, outperforming classical methods in this matter. 


State-of-the-art deep learning-based feature extractors  typically compute the features and descriptors simultaneously. For example, \ac{LIFT} \cite{yi2016liftdescriptor}, used in LIFT-SLAM \cite{bruno2021lift}, keeps full end-to-end differentiability when combining feature detection, orientation estimation, and feature description. The training uses 3D points generated by a \ac{SfM} that runs on images at different scales, viewpoints, and lighting conditions.
Superpoint \cite{2018superpoint} operates on full-sized images, rather than just on patches, and is pre-trained with a synthetic dataset first and then automatically adapted to a real unlabeled domain. Unsuperpoint \cite{christiansen2019unsuperpoint} uses a self-supervised approach, making it end-to-end trainable.
\begin{figure}[b!]

    \centering
    \smallskip
    \includegraphics[scale=1]{figs/paper1-survey/Front-end/2-photometric_error.pdf}
    \caption[Diagram showcasing how direct SLAM bases the tracking on optimizing the photometric error]{Direct SLAM bases the tracking on optimizing the photometric error. Given two image frames $I_{k-1}$ and  $I_k$, the 3D point $P_i$ is projected into  $p^{k-1}_i$ and $p^k_{i}$, respectively. The photometric error is the intensity difference between $p^{k-1}_i$ and $p^k_{i}$, obtained by projecting $I_k$ onto $I_{k-1}$.}
    \label{fig:photometric}
\end{figure}


\subsubsection{Direct methods}
\label{sec:frontend:directmethods}
Direct methods, introduced in 2007 by DTAM \cite{klein2007dtam}, use image intensities rather than sparse features. They directly compare the pixel intensities between consecutive image frames, so that the pixel values at $p^{k-1}_i$ and at $p^k_{i}$, corresponding to the same 3D point $P_i$ projected in the image planes $I_{k-1}$ and $I_k$, are the most similar to each other. This comparison is defined as a photometric error, which can be formulated as follows:
\begin{equation}
    r_{ph}(P_i,T_{k,k-1},\xi) = \sum_i I_{k-1}(p^{k-1}_i) - I_k\Bigl(\pi (p^{k}_i,T_{k,k-1},\xi)\Bigr)
\end{equation}
with $\pi$ a function warping the pixels in $I_k$ onto the image plane $I_{k-1}$.
Direct methods define tracking as an optimization problem. In this case, the optimization minimizes the aggregated photometric error around a parameter that can either be the camera transform between reference to candidate frame $T_{k-1,k} \in$ $SE(3)$ or the inverse depth map $\xi_k:\Omega\rightarrow \mathbb{R}$, which is directly proportional to the disparity. To each pixel $p^{k}_i$ it corresponds an inverse depth value $d_i = \xi_k(p^{k}_i)$. The use of an inverse depth formulation has the advantage of removing ambiguity, and increasing precision \cite{okutomi1993stereomatch} and, therefore, is more commonly used.

LSD-SLAM \cite{engel2014lsd} implements a Gauss-Newton minimization, iteratively down-weighting large residuals that occlusions and reflections might cause, hence making the tracking more robust against them. GCP-SLAM \cite{zhang2019gcpslam} up-weights the terms with high confidence and down-weights those with high uncertainty.
Some implementations also include the reprojection residual in the optimization \cite{zhang2019gcpslam} \cite{zhao2019robust}. 

The photometric error is a strongly non-convex function; therefore, the minimization can get easily stuck at a local minimum. The two key concepts to consider in addressing this issue are the pixel gradient and the image pyramid.

The pixel gradient defines the intensity change on a pixel. Higher gradients correspond with higher intensity changes and better convergence of the minimization. 
The consideration of the pixel gradient in selecting pixels for minimizing the photometric error yields the following classification of direct methods: sparse, semi-dense and dense.
Sparse direct methods only include sparse keypoints in the computation of the photometric error. In direct SLAM, the descriptors are not computed. 
Semi-dense methods aim to trade off the computational efficiency of sparse methods versus the robustness of dense methods. They reduce the number of pixels tracked by only selecting those with high gradients \cite{forster2014svo}. That is the case for LSD-SLAM \cite{engel2014lsd}, SVO \cite{forster2014svo}, and DSO \cite{engel2017dso}. SVO improves the convexity by computing the photometric error over a patch of pixels rather than a per-pixel computation. 
Dense methods use all pixels in the image, making them more computationally expensive and slower to converge. However, these methods allow computing a complete dense 3D map.  


Dense and semi-dense methods are more robust to occlusions or blur due to rapid motion. However, they are more prone to fail under large baselines. Some direct methods handle this issue with an image pyramid, also referred to as coarse-to-fine approach, matching at low resolutions first and then iteratively refining at higher resolutions. Most direct SLAM algorithms follow this approach \cite{klein2007dtam,engel2014lsd,forster2014svo,engel2017dso}. SVO optimizes the photometric error in the coarsest levels until reaching enough convergence and then initialises feature alignment that considers changes in illumination. DSO uses the coarsest pyramid levels to initialise the depth estimate and relocalisation.



\subsubsection{Indirect methods}
\label{sec:frontend:indirectmethods}
The first feature-based monocular visual SLAM was introduced in MonoSLAM \cite{davison2007monoslam}. Feature-based SLAM tracks sparse salient features between adjacent images.
When tracking sparse features, it is desired to find a uniform distribution of the detected features over the image. This way, matching will still be possible in the presence of occlusions, fast movements, or any other situation involving loss of information.
ORB-SLAM \cite{mur2015orb} addresses the search for that uniform distribution by performing a grid search such that at least five features are detected in each cell. VINS-Mono \cite{qin2018vins-mono} forces a minimum separation of pixels between two neighboring features.

\begin{figure}[tpb]

    \centering
    \smallskip
    \includegraphics[scale=1]{figs/paper1-survey/Front-end/3-reproj_error.pdf}

    \caption[Diagram showcasing how sparse SLAM optimizes the projection error for tracking]{Sparse SLAM optimizing the reprojection error for tracking. Two matched points $p^{k-1}_i$ and $p^{k}_i$ are triangulated onto the estimated 3D point $\hat{P}_i$, which is then reprojected as $\hat{p}^{k-1}_i$ and $\hat{p}^{k}_i$. The error is the difference between reprojected and original position of the points.}
    \label{fig:reprojection}
\end{figure}

One standard approach for feature matching would be to detect features, compute their descriptors in two or more views, and then match the closest features in the descriptor space.
These methods are robust to large baselines and view changes. Nevertheless, optical flow methods like the \ac{KLT} tracker are more accurate and therefore better suited for tracking \cite{qin2018vins-mono}. \ac{KLT} approaches match features by minimizing the sum of squared intensity differences \cite{lucas1981klt1}\cite{tomasi1991klt2}.
Another alternative for matching is to assume an affine transformation between adjacent frames and use that transformation to search for the features from the first frame in the second frame. ORB-SLAM's \cite{mur2015orb} pipeline, based on that of \ac{PTAM}  \cite{klein2007ptam}, uses a constant velocity model. It fastens the calculation because the subset of pixels included in it decreases, but it could fail under wrong model estimations or significant imaging changes.
Similarly, VITAMIN-E \cite{Yokozuka_2019vitamine} fits the affine transformation according to the dominant flow estimated from coarse feature pairs. In addition to that, VITAMIN-E performs optimization on the local extrema tracking (rather than descriptors), which avoids falling into a local solution by using the dominant flow estimate.

Classic matching implementations assume static feature points and low or no dynamic feature points. The dynamic feature points are treated then as outliers, which are removed from the matches using outlier rejection algorithms like \ac{RANSAC} \cite{fischler1981RANSACpnp}. Deep learning has introduced semantic methods that identify those dynamic points. This way, they can be removed from the motion computation. Approaches like CubeSLAM \cite{yang2019cubeslam} and PO-SLAM \cite{li2021po-slam} use YOLO \cite{redmon2017yolo9000} to obtain the bounding boxes for dynamic objects. YOL-SLAM \cite{mengcong2021yol-slam} decreases the number of layers in YOLO to improve the processing speed. However, one of the main difficulties for these approaches lies in the ambiguity near the boundaries. The boundaries are typically high-gradient areas, where numerous features might be detected. Methods based on segmentation networks take the boundaries out of the computation.
DS-SLAM\cite{yu2018ds-slam} and SOF-SLAM\cite{cui2019sof-slam} use the segmentation network SegNet \cite{badrinarayanan2017segnet}. DynaSLAM\cite{dynaslam18} uses Mask R-CNN \cite{he2017maskRCNN}, combining it with the geometric consistency for the segmentation. It also fills the occluded background with static information from previous views. RDS-SLAM \cite{liu2021rds-slam} uses BlitzNet \cite{dvornik2017blitznet} for the segmentation mask, with the semantic part as a new thread separated from the tracking, allowing better real-time performance.

Once the features have been matched, the motion can be estimated. This extra step of estimating the motion from the geometry constraints, rather than directly estimating it from the image intensities, gives this family of methods the qualification of "indirect".
 For monocular visual SLAM, there are two cases for motion extraction: 2D-to-2D, and perspective-n-point (PnP), also referred to as 3D-to-2D.
 
In {2D-to-2D}, and assuming calibrated cameras, the essential matrix can be derived from the matches using the epipolar constraint. It follows that, for the matched points $p^{k-1}_i \leftrightarrow p^{k}_i$,    \begin{equation}
        \left[ \tilde{p}^{k}_i \right]^T E \tilde{p}^{k-1}_i = 0
    \end{equation}
    in homogeneous coordinates. The essential matrix $E$ has five degrees of freedom: It can be solved with at least 5 points \cite{nister2004-5point}, although the most classic implementation is the eight-point-algorithm \cite{hartley1997-8point}.
    Rotation and translation can be derived from it up to an scale factor.
    \begin{equation}
        E_k = [t_k]_\times R_k, 
    \end{equation}
    with $[t]_\times$ the matrix representation of the cross product of $t$. $R_k$ and $t_k$ are obtained from the \ac{SVD} of $E$. For further details on the theory behind epipolar geometry, we refer the reader to the book by Hartley and Zisserman \cite{heyden2005multiplevgHyZ}. This method suffers from instability under small translations and is unsolvable under pure rotations, thus is not usually implemented in the front-end. Instead, it has been used for map initialization \cite{butt2020epipolarinitializ} or map point verification \cite{mur2015orb}. 
PnP is the problem of estimating the relative pose between a calibrated camera and $n$ 3D points in the scene, given the correspondences of their 2D projections \cite{fischler1981RANSACpnp}. It requires matched features across three views, which requires an initialization in the monocular case. The minimal solution is given by the Perspective-three-point (P3P) algorithm \cite{gao2003p3p}. ORB-SLAM \cite{mur2015orb} (and by extension, all approaches using the ORB-SLAM front-end) uses EPnP, which can use any amount of points \cite{lepetit2009epnp}. DF-VO \cite{zhan2019dfvo} uses a learnt optical flow for the 2D correspondences and a learnt depth for the 3D representation.






Once the first motion estimate is obtained, the measurements are refined in an optimization step, often referred to as local bundle adjustment. 
It consists of minimizing the reprojection error, resulting from projecting the 3D points into the image planes and comparing the reprojection with the original position of the feature points, as represented in Fig. \ref{fig:reprojection}. 
The reprojection error for the matched points $\textbf{p}^{k-1}_i \leftrightarrow \textbf{p}^{k}_i$, can be formulated as a least-squares problem:
\begin{equation}
     r_{r}(P_i) = \sum_id(p^{k-1}_i,\hat{p}^{k-1}_i)^2+d(p^{k}_i,\hat{p}^{k}_i)^2,
\end{equation}
with $d$ the distance function. 





\subsection{Loop closure detection and relocalization}
\label{section:frontend:LoopClosure-relocalization}
The loop closure thread aims to find a revisited area in the front-end and then optimize the map and trajectory in the back end.
Similarly, if the robot loses track of its trajectory, it can relocate itself in the previously built map by detecting a loop. Consequently, loop detection is one of the essential modules for the accuracy and robustness of \ac{SLAM}.

\begin{figure}[tb]
  \centering
  \includegraphics[scale=1.1]{figs/paper1-survey/Front-end/bow.pdf}
  \caption[The visual Bag of Words framework]{The BoW framework represents the image by extracting
  and clustering the descriptor vectors. Each cluster represents a visual word. The histogram of words comprises the BoW vector that describes the image's appearance.}
  \label{fig:survey:bow}
\end{figure}


The key to loop closure detection lies in calculating the similarity between images. An intuitive approach would be determining a loop according to the number of correct matches detected in feature matching. However, searching through the whole database will be computationally expensive and memory inefficient as the map grows.
Instead, loop detection is divided into three steps: image representation, candidate selection, and loop verification.

One classical framework for loop detection is \ac{BoW}, first proposed in \cite{sivic2003bowzisserman} and outlined in Fig. \ref{fig:survey:bow}. In this framework, image features and their descriptors are extracted for the image representation. ORB-SLAM \cite{campos2021orb} uses DBoW2\cite{galvez2012dbow2}  with the same ORB features as in tracking to achieve real-time performance. Other approaches leveraging the ORB-SLAM backend also use DBoW2, namely LIFT-SLAM \cite{bruno2021lift}, RDS-SLAM \cite{liu2021rds-slam}, SOF-SLAM \cite{cui2019sof-slam}, and DS-SLAM \cite{yu2018ds-slam}. Deep learning features also find their application in loop closure detection, not only with patch descriptors \cite{LCDwithSuperpoint}, but also with global descriptors \cite{li2020dxslam,xu2021esa-vlad}.
The descriptors are then clustered and stored in a codebook with a clustering algorithm like k-means. Each cluster represents a visual word of the codebook. Additionally, DBoW2 \cite{galvez2012dbow2} repeats this operation for each cluster to generate a hierarchical tree. The clustering of descriptors is time-consuming and usually trained offline. The work in iBoW-LCD \cite{garcia2018ibow}, implemented in 
OV$^2$SLAM \cite{ferrera2021ov2slam}, builds the vocabulary trees incrementally so that it constantly adapts to the current environment without needing offline training.

The candidate frame selection retrieves candidates according to the nearest neighbours to the queried frame. 
Most implementations rely on image sequences rather than single images.
DBoW2 \cite{galvez2012dbow2} chooses consistent candidates within a time interval before entering the verification step.

BoW portrays an appearance-based framework and does not encode spatial information. Therefore, loop verification applies a geometry consistency check. It is done by finding a fundamental matrix in DBoW2 \cite{galvez2012dbow2} and also in ESA-VLAD \cite{xu2021esa-vlad} between the queried and candidate images with enough inliers using RANSAC. ORB-SLAM2 \cite{campos2021orb} additionally applies place recognition to three consecutive frames to improve recall. DXSLAM \cite{li2020dxslam} and OV$^2$SLAM \cite{ferrera2021ov2slam} apply the standard RANSAC and PnP process.
In LRO \cite{ma2021loopsuperpoint2} the loop is verified according to the topological structure between groups of keypoints, considering rotation, scaling, and deformation.



Computationally expensive SLAM approaches sometimes sacrifice the loop closure thread to speed up the algorithm. That is the case of direct slam approaches \cite{klein2007dtam,zhang2020vdo,li2021po-slam}, or in those that incorporate deep learning in some of their processes \cite{tateno2017cnnslam,dynaslam18,fan2022blitz-slam}. However, some deep learning approaches utilize their capabilities in the loop closure thread. That is the case of some semantic approaches that detect the loop when reobserving an object in the scene \cite{bavle2020vps-slam,parkhiya2018semanticloopclosure}.

Direct approaches like LSD-SLAM \cite{engel2014lsd}, the subsequent DSO \cite{engel2017dso} and GCP-SLAM \cite{zhang2019gcpslam} use the appearance-based method FAB-MAP \cite{cummins2008fabmap} for detecting a loop, which is verified with a statistical test.  
The work in LDSO \cite{gao2018ldso} extends the capabilities of DSO by using the classic BoW framework and the same features as in the front-end.
